{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "502b959c-28d1-42b4-a143-386fa4e809c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9OPrsjfRfsMn"
   },
   "source": [
    "# Create Bronze Notebook - Data Ingestion\n",
    "\n",
    "### CSIS4495-050: Applied Research Project\n",
    "\n",
    "#### End-to-End Data Engineering Solution for HR Analytics\n",
    "\n",
    "Group:\n",
    "- Bruno do Nascimento Beserra\n",
    "- Jay Clark Bermudez\n",
    "- Matheus Filipe Figueiredo\n",
    "\n",
    "Instructor: Dr. Bambang Sarif\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Description:\n",
    "\n",
    "This project simulates the evolution of a mid-sized company with 5,000 employees over a period of seven years. To build the initial workforce, we used a Kaggle dataset containing employee information and extracted a representative sample to serve as our company’s employee force.\n",
    "\n",
    "To showcase our data pipeline solution built with state-of-the-art techniques. We designed a realistic simulation environment that captures key workforce dynamics over time. Throughout the seven-year period, employees may experience promotions, change teams, or leave the company. In parallel, the company will continuously hire new employees, based in their information from the main dataset to keep the workforce evolving.\n",
    "\n",
    "In this notebook, we ingest raw CSV files stored in a structured folder hierarchy (`year/month/data.csv`) into a Delta table named **`bronze_data`**, adding ingestion metadata for traceability and enabling incremental data processing.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Step by Step:\n",
    "\n",
    "- Import Libraries and Dataset\n",
    "- Define Configurations\n",
    "- Check root folder for new files\n",
    "- Read data\n",
    "- Create ingestion_timestamp column\n",
    "- Save Bronze Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3bba89-a066-4647-ae95-df25acacd80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "beKXokjTfKbd"
   },
   "outputs": [],
   "source": [
    "# Import Libraries and Start Spark Session\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f87fd4-c363-4019-a0ea-699f37c5190e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define path with dataset\n",
    "base_path = \"/Volumes/workspace/my_data/historical_data/unzipped/historical_data\"\n",
    "\n",
    "metadata_file = f\"{base_path}/metadata.txt\"\n",
    "bronze_table = \"workspace.applied_research_bronze.hr_bronze_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24976fd3-f208-48d3-b084-407679446499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    prev_files = (\n",
    "        dbutils.fs.head(metadata_file, 1000000)\n",
    "        .strip()\n",
    "        .split(\"\\n\")\n",
    "    )\n",
    "except Exception:\n",
    "    prev_files = []\n",
    "\n",
    "ingested_files = set(f for f in prev_files if f.strip() != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19719112-9b6b-4e0c-9d10-0dff25d33ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_csv_files(base_path):\n",
    "    files = []\n",
    "    for year in [f.path for f in dbutils.fs.ls(base_path)]:\n",
    "        for month in [m.path for m in dbutils.fs.ls(year)]:\n",
    "            for f in dbutils.fs.ls(month):\n",
    "                if f.path.endswith(\".csv\"):\n",
    "                    files.append(f.path)\n",
    "    return files\n",
    "\n",
    "all_csv_files = list_all_csv_files(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99064a5-a446-4a94-8093-de4b5c13210f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_files = [f for f in all_csv_files if f not in ingested_files]\n",
    "\n",
    "if not new_files:\n",
    "    print(\"No new files to ingest today.\")\n",
    "else:\n",
    "    print(f\"Found {len(new_files)} new files:\")\n",
    "    for nf in new_files:\n",
    "        print(\" -\", nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bef6c1-ab17-4674-b416-fae95ad46aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if new_files:\n",
    "    df_new = (\n",
    "        spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .csv(new_files)\n",
    "    )\n",
    "    if \"ingestion_timestamp\" not in df_new.columns:\n",
    "        df_new = df_new.withColumn(\"ingestion_timestamp\", f.current_timestamp())\n",
    "    else:\n",
    "        print(\"'ingestion_timestamp' already exists in the data — skipping column creation.\")\n",
    "\n",
    "    # Append to Bronze Delta table\n",
    "    df_new.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table)\n",
    "\n",
    "    # Update metadata file with the newly ingested file paths\n",
    "    all_ingested = list(ingested_files.union(new_files))\n",
    "    dbutils.fs.put(metadata_file, \"\\n\".join(all_ingested), overwrite=True)\n",
    "\n",
    "    print(f\"Ingested {len(new_files)} new files and updated metadata.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7473979426230471,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "applied_research_bronze_notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
