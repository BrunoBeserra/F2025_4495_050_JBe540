{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54c3f1fa-96a0-496f-9bc5-34ed1ef05f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Gold Notebook with Kimbal Architecture\n",
    "\n",
    "### CSIS4495-050: Applied Research Project\n",
    "\n",
    "Group:\n",
    "- Bruno do Nascimento Beserra\n",
    "- Jay Clark Bermudez\n",
    "- Matheus Filipe Figueiredo\n",
    "\n",
    "Instructor: Dr. Bambang Sarif\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884d81ad-2cfa-4f28-9a81-b5f9565a3d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType, DoubleType, BooleanType\n",
    "import os\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8cd58ee-8017-41a8-9118-4ee03c7a1546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "gold_path = \"workspace.default\"\n",
    "silver_table = \"workspace.default.hr_silver_data\"\n",
    "\n",
    "dim_tables = [\n",
    "    \"dim_department_gold\",\n",
    "    \"dim_job_title_gold\",\n",
    "    \"dim_location_gold\",\n",
    "    \"dim_status_gold\",\n",
    "    \"dim_work_mode_gold\",\n",
    "    \"dim_job_level_gold\"\n",
    "]\n",
    "\n",
    "fact_table = \"fact_table_gold_hr_data\"\n",
    "\n",
    "# Define schema for dim_tables\n",
    "dim_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for fact table\n",
    "fact_schema = StructType([\n",
    "    StructField(\"employee_id\", StringType(), True),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"job_title_id\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True),\n",
    "    StructField(\"location_id\", IntegerType(), True),\n",
    "    StructField(\"performance_rating\", IntegerType(), True),\n",
    "    StructField(\"experience_years\", IntegerType(), True),\n",
    "    StructField(\"status_id\", IntegerType(), True),\n",
    "    StructField(\"work_mode_id\", IntegerType(), True),\n",
    "    StructField(\"annual_salary\", DoubleType(), True),\n",
    "    StructField(\"job_level_id\", IntegerType(), True),\n",
    "    StructField(\"ingestion_timestamp\", DateType(), True),\n",
    "    StructField(\"data_hash\", StringType(), True),\n",
    "    StructField(\"start_effectivity_date\", DateType(), True),\n",
    "    StructField(\"end_effectivity_date\", DateType(), True),\n",
    "    StructField(\"is_current\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "dim_mapping = {\n",
    "    \"department\": \"dim_department_gold\",\n",
    "    \"job_title\": \"dim_job_title_gold\",\n",
    "    \"location\": \"dim_location_gold\",\n",
    "    \"status\": \"dim_status_gold\",\n",
    "    \"work_mode\": \"dim_work_mode_gold\",\n",
    "    \"job_level\": \"dim_job_level_gold\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0978239c-8b99-42aa-945e-31d007b83ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def table_exists(table_name):\n",
    "    return spark.catalog.tableExists(f\"{gold_path}.{table_name}\")\n",
    "\n",
    "def dim_create_update(fact_df, col_name, table_name):\n",
    "    if table_exists(table_name):\n",
    "        dim_df = spark.table(f\"{gold_path}.{table_name}\")\n",
    "    else:\n",
    "        dim_df = spark.createDataFrame([], \"id int, name string\")\n",
    "    \n",
    "    fact_vals = fact_df.select(col_name).where(f.col(col_name).isNotNull()).distinct().withColumnRenamed(col_name, \"name\")\n",
    "    new_vals = fact_vals.join(dim_df, \"name\", \"left_anti\")\n",
    "\n",
    "    if new_vals.count() > 0:\n",
    "        max_id = dim_df.agg(f.coalesce(f.max(\"id\"), f.lit(0))).collect()[0][0]\n",
    "        window = Window.orderBy(\"name\")\n",
    "        new_vals = new_vals.withColumn(\"id\", f.row_number().over(window) + max_id)\n",
    "        dim_df = dim_df.unionByName(new_vals)\n",
    "    dim_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{gold_path}.{table_name}\")\n",
    "    return dim_df.select(\"id\", \"name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da47c9d-87b1-434a-966d-f2ab3a9aef05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for dim in dim_tables:\n",
    "    full_path = f\"{gold_path}.{dim}\"\n",
    "    if not table_exists(dim):\n",
    "        empty_df = spark.createDataFrame([], schema=dim_schema)\n",
    "        empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_path)\n",
    "    else:\n",
    "        print(f\"{full_path} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c069bea6-bcc6-40d2-8bef-4a0f89764a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(fact_table):\n",
    "    empty_fact_df = spark.createDataFrame([], fact_schema)\n",
    "    empty_fact_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{gold_path}.{fact_table}\")\n",
    "    print(f\"Created fact table: {gold_path}.{fact_table}\")\n",
    "else:\n",
    "    print(f\"Fact table already exists: {gold_path}.{fact_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55959a8-faa0-42d3-a504-a470e59ca322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_ingestion_timestamp = None\n",
    "\n",
    "if table_exists(fact_table):\n",
    "    last_timestamp_df = spark.table(fact_table).select(f.max(\"ingestion_timestamp\").alias(\"last_timestamp\"))\n",
    "    last_ingestion_timestamp = last_timestamp_df.collect()[0][\"last_timestamp\"]\n",
    "\n",
    "if last_ingestion_timestamp is not None:\n",
    "    df_fact = spark.sql(f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {silver_table}\n",
    "        WHERE ingestion_timestamp > '{last_ingestion_timestamp}'\n",
    "    \"\"\")\n",
    "else:\n",
    "    df_fact = spark.table(silver_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b67f03-7199-4538-8b64-da4e52b1d716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fact.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c1c85a-4eb9-40ce-93f6-828eb26174b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col, dim_table in dim_mapping.items():\n",
    "    dim_df = dim_create_update(df_fact, col, dim_table)\n",
    "    df_fact = df_fact.join(dim_df, df_fact[col] == dim_df[\"name\"], \"left\") \\\n",
    "                     .drop(col, \"name\") \\\n",
    "                     .withColumnRenamed(\"id\", f\"{col}_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d64e386f-11e8-48ca-b193-bbe48c3e027d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fact = df_fact.withColumn(\n",
    "    \"job_level_id\",\n",
    "    f.when(f.col(\"job_level_id\").isNull(), f.lit(5)).otherwise(f.col(\"job_level_id\"))\n",
    ")\n",
    "\n",
    "# Append new rows\n",
    "if df_fact.count() > 0:\n",
    "    df_fact.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{gold_path}.{fact_table}\")\n",
    "else:\n",
    "    print(\"No new fact rows to insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09cbb3a-18de-47c2-98f2-7819edc46de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061d5389-7fb5-487c-941a-6526f84a74c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_level_order = {\n",
    "    \"Specialist\": 1,\n",
    "    \"Analyst\": 2,\n",
    "    \"Manager\": 3,\n",
    "    \"Principal\": 4,\n",
    "    \"Executive\": 5\n",
    "}\n",
    "\n",
    "dim_job_level = spark.table(f\"{gold_path}.dim_job_level_gold\")\n",
    "mapping_expr = f.create_map([f.lit(x) for x in sum(job_level_order.items(), ())])\n",
    "\n",
    "dim_job_level = dim_job_level.withColumn(\"job_level_order\", mapping_expr[f.col(\"name\")])\n",
    "\n",
    "# Save back to delta table\n",
    "dim_job_level.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{gold_path}.dim_job_level_gold\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "applied_research_gold_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
